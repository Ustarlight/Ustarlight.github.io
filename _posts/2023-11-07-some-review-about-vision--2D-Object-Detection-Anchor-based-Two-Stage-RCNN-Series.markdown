---
layout: post
title:  "2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series"
date:   2023-11-07 10:42:45 +0800
categories: share
---

**基于锚框的物体检测**

给定一张图片，其中包含待检测物体（如下图中绿色框选出来的人脸），基于锚框的物体检测主要分为3个步骤：

1. 锚框设计：对锚框进行设计，即设计锚框的大小、长宽比和间隔（隔多少像素铺一个锚框）这三个参数；
2. 锚框分类：对锚框进行分类，判断哪些锚框属于前景（待检测物体），哪些锚框属于背景；
3. 锚框回归：对属于前景的锚框进行回归（调整锚框的位置、大小）。

锚框分类与锚框回归这两个步骤统称为锚框预测，锚框预测这个能力是需要通过神经网络学习出来的。既然需要神经网络学习，就需要一个监督信号，这里引入锚框匹配，将初始铺设好的锚框与GT进行匹配，区分哪些是正样本哪些是负样本（一般通过判断锚框与GT的IOU是否大于一定阈值来区分）。如何监督分类和回归？

从上述流程中可以简单归纳一下什么是锚框，锚框本质上就是图片中的一块矩形区域。检测流程可以简述为首先在图片中铺上大大小小的锚框，即选中图片中长宽比、大小、位置各不相同的区域；采用算法对该区域进行预测（锚框分类+锚框回归），得到精确的包含待检测物体的区域。

整个检测过程可以级联的进行重复，将锚框回归的结果当做初始铺设好的锚框，再次进行分类与回归，从而达到提升分类、回归精度的效果。这个过程属于多阶段物体检测算法，具有精度高的优点，但是缺点就是效率不如单阶段法高。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 11.11.44.png)

**RCNN系列**

R-CNN在2013年年底就已经挂出来了，可以算是第一篇采用深度学习进行物体检测的算法；2015年4月，作者提出了Fast R-CNN，对R-CNN进行了改进，优化了检测效率，发表在了ICCV 2015上，该工作属于奠基性工作，确定了一些检测流程的框架；2015年6月，提出了Faster R-CNN，对Fast R-CNN又进行了改进，发表在了NIPS 2015上，这篇工作属于集大成者，基本上确定了基于锚框的物体检测算法的检测流程，后续大多数多阶段检测算法都是基于Faster R-CNN改进的；又过了一年多，FPN被提出了，主要是针对Faster R-CNN中的特征提取模块进行了改进，后续也经常被大家所使用；2017年3月，Mask R-CNN被提出，该工作主要是在R-CNN系列的基础上，针对实例分割任务对网络进行了修改。

由于Faster R-CNN是R-CNN系列的集大成者，后续多阶段检测算法都是基于该工作改进的，因此只要搞清楚Faster R-CNN，基本上目前绝大多数的多阶段物体检测算法都能整明白了。

接下来简要介绍一下R-CNN和Fast R-CNN，再重点介绍Faster R-CNN。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 11.14.43.png)

**R-CNN**

R-CNN主要分为6个检测步骤：

1. 输入图像：输入一张待检测的图像；
2. 生成候选区域：使用Selective Search算法，在输入图像上生成约2k个候选区域（锚框）；
3. 处理候选区域：在输入图像上裁剪出每个候选区域，并缩放到227*227大小；
4. 特征提取：将裁剪出的每个候选区域，输入到CNN网络中，得到一定维度（如4096维）的特征；
5. 类别判断：将从每个候选区域提取出的特征送入各自的SVM分类器（若有80类待检测物体，则有80个人工设计的SVM分类器），判断候选区域是否属于该类；
6. 位置精修：使用回归器修正候选框的位置（这里采用的是传统图像处理方法，将候选区域放大到一定尺寸，对候选区域进行边缘检测，把所有边缘的最小包围框作为最终的预测结果）。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 11.25.41.png)

在生成候选区域时用到的Selective算法的大致流程为：采用传统的图像处理算法（如区域生长）产生图像的初始分割区域，然后根据颜色、纹理、大小、形状等计算相邻区域的相似度，根据相似度对相邻区域进行合并。一张输入图像可以得到多张分割结果，对每张分割图像中的分割区域计算外接矩形框，将这些矩形框作为候选区域（锚框）。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 11.32.53.png)

相比传统算法，R-CNN中因为加入了CNN，生成了更多维的特征，检测精度得到了大幅的提升，但是速度太慢，主要原因有三点：

1. 生成候选区域时，采用的Selective Search算法非常耗时；
2. 一张图像中有~2k个候选区域，需要使用~2k次CNN提取特征，存在大量重复计算；
3. 特征提取、图像分类、边框回归是三个独立的步骤，要分别训练，测试效率也较低。

**Fast R-CNN**

Fast R-CNN的核心在于针对上述提到的R-CNN速度慢的原因中的第2、3项进行了改进，先看一下Fast R-CNN的检测流程：

1. 输入图像：输入一张待检测的图像；
2. 生成候选区域：使用Selective Search算法，在输入图像上生成大约~2k个候选区域（锚框）；
3. 特征提取：将整张图像输入到CNN中提取特征，得到特征图像；
4. 候选区域特征映射：将原图像的候选区域映射到特征图像上，利用ROI Pooling**分别**生成每个候选区域的特征；
5. 候选区域分类和回归：采用映射出来的并经过ROI Pooling的特征，对每个候选区域进行分类和回归。

注：步骤1、2与R-CNN完全相同，都是采用Selective Search生成待检测的候选区域；步骤3中将整张图像输入CNN提取特征，后续特征计算都是在特征图上进行的，解决了R-CNN中需要使用~2k次CNN导致大量重复计算以至于速度慢的问题；步骤3、4、5将特征提取、锚框分类、锚框回归放到了同一个网络中，提升了训练和测试效率。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 11.43.57.png)

**ROI Projection**

在步骤4中，首先将原图像的候选区域映射到特征图像上，比如原图像上的候选区域大小为200x145，特征图的下采样倍数为32倍，那么特征图上候选区域的大小就是⌊200/32⌋=6 x ⌊145/32⌋=4，其X、Y的坐标位置为X=⌊296/32⌋=9、Y=⌊192/32⌋=6，最终映射到特征图上的ROI区域如下图中的橙色区域所示。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 13.44.54.png)

**ROI Pooling**

随后用到了ROI Pooling，进一步对每个候选区域的特征做处理，其核心思想是利用**特征采样**，将不同空间大小的特征变成空间大小一致的特征。为什么要用ROI Pooling把不同大小的特征变成固定大小呢？原因有二：1、ROI Pooling后续跟的是全连接层，要求输入的维度是固定的；2、各个候选区域的大小是不同的，对应的特征图上的特征大小也是不一样的，经过ROI Pooling将不同大小的特征变成相同大小的特征之后，可以把这些特征组成batch，提升处理效率。

ROI Pooling的操作过程如下，假设有一个5x7维的特征，想将其变为2x2维的特征，ROI Pooling就是先将特征分割成2x2个子区域，令每个子区域中的最大值代表该区域，最终得到2x2维的特征。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-07 16.25.11.png)

Fast R-CNN是端到端训练的，并且由于减少了大量的重复计算，其速度比R-CNN提升了200多倍，而且精度更高。此时采用Selective Search进行候选区域生成是制约效率的最大瓶颈（耗时约2s），为了去除这一瓶颈，Faster R-CNN诞生了。

**Faster R-CNN**

Faster R-CNN提出了RPN，取代了之前最耗时的Selective Search，将RPN嵌入到了Fast R-CNN的网络里，使得网络可以端到端训练，基本上确定了锚框的检测流程。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 10.50.14.png)

**RPN (Region Proposal Network)**

讲一下RPN的原理和流程。先来看一个比较简单的情况，输入一张4x4的图像如下图所示，图像中的绿色框选出来的区域为待检测目标，经过一个size=2×2、stride=2、padding=0、channel=1的卷积核卷积后，生成一个2×2的特征图，在特征图上的每一个特征都可以在原图像上找到对应的区域与区域中心，原图上对应的这个区域就叫做**感受野**，区域的中心就是感受野的中心。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 11.06.31.png)

特征图上的每个特征都可以关联N个锚框，锚框的中心位于相应特征对应感受野的中心，锚框的尺寸大小和长宽比则是人为设定好的（这里暂且设定为每个特征关联1个大小为1、长宽比为1:1的锚框）。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 11.09.50.png)

对锚框进行正负样本的划分，认定最佳匹配或者与目标框的IOU大于
$$
\theta_+
$$
的锚框为正样本，与目标框的IOU小于
$$
\theta_-
$$
的锚框为负样本，在两个阈值之间的为忽略样本。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 11.14.23.png)

接下来进行锚框分类，目前我们已经知道了锚框的真实分类标注，即哪些锚框是正样本，哪些锚框是负样本，然后就可以训练RPN使得RPN具有分类锚框的能力。对特征图进行size=1x1、channel=2的卷积，对于每个特征来说，通过卷积后在相应位置都能得到两个值，可以认为一个是特征对应的锚框作为背景（负样本）的概率，一个是作为前景（正样本）的概率，将输出的值与真实标注计算Softmax Loss，采用BP即可训练网络。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 11.22.50.png)

然后进行锚框的回归，由于初始设置的锚框与真实标注之间的差异很大，需要通过锚框回归使两者尽量一致。既然用到了回归，那么就需要有一个回归的目标值。回归目标值的计算公式如下图所示，其中需要注意的一点就是在计算位置差异
$$
\triangle x_c^* + \triangle y_c^*
$$
时，分别**除以了w和h进行归一化**，这么操作的原因是大的锚框和大的真实标注之间的差值较大、小锚框和小的真实标注之间的差值较小，通过除以锚框的宽、高，可以将不同大小的锚框和真实标注之间的差值约束在一个范围内。



![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 11.54.42.png)

有了锚框回归的目标值之后，就可以训练网络使其具有锚框回归的能力。对特征图进行size=1x1，channel=4的卷积，对于每个特征来说，卷积后在相应位置都能得到4个预测值，将其分别作为特征对应的锚框与真实标注框的偏差预测值
$$
\triangle x_c, \triangle y_c, \triangle w, \triangle h
$$
将预测值与真实标注计算SmoothL1 Loss，采用BP即可训练网络。将预测
$$
\triangle x_c, \triangle y_c, \triangle w, \triangle h
$$
分别加上锚框原本的位置、大小，即得到预测框的位置和大小。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 14.06.33.png)

上面介绍了将特征图上的每个特征分别关联单个锚框时，如何进行分类与回归。关联多个锚框的流程也类似，增加分类和回归的卷积层的channel数量即可。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 14.10.04.png)

在理解简单情况下的锚框分类与回归后，来看看真正的RPN长什么样子。

给定一副图像，经过VGG16/ResNet101等Backbone后，RPN选取下采样倍数为16的特征层作为后续分类和回归的输入特征；特征层中的每一个特征关联9个锚框，大小分别设置为128、256、512，长宽比分别设置为0.5、1、2。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 14.12.52.png)

讲了这么多，总结一下如何利用RPN来生成候选区域。

1. 特征提取：输入一幅图像，采用VGG16/ResNet101等Backbone，得到下采样倍数为16倍的特征图，并且特征图上的每个特征都可以在原图中找到对应的感受野；
2. 关联锚框：为每个特征关联大小、长宽比各不相同的9个锚框，并将特征图作为输入，进行后续的锚框分类与锚框回归；
3. 生成候选区域：根据锚框分类中预测的锚框前景得分，得出属于前景的锚框；
4. 调整候选区域：根据锚框回归中预测的锚框偏移量，调整前景锚框的位置和长宽；
5. 筛选候选区域：采用NMS去掉冗余的候选区域，得到最终候选区域；
6. 增强候选区域（仅训练时采用）：将真实标注框也作为最终候选区域，放入候选区域子集中，增加训练样本量。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 14.17.20.png)

**RPN + Fast R-CNN = Faster R-CNN**

Faster R-CNN就是将Fast R-CNN中的selective search换成了RPN，其他和Fast R-CNN一样。总结一下Faster R-CNN的整体流程：

1. RPN步骤：

​	（1）将整张图像传入Backbone提取特征；

​	（2）选择下采样倍数为16的特征层作为检测层；

​	（3）在检测层的每个特征上关联一系列（共9个）大小、长宽比各不相同的锚框，对应原图像的不同区域； 

​	（4）对锚框进行二分类和回归得到若干个候选区域。

2. Fast R-CNN步骤

​	（1）将候选区域映射到检测层上，采用RoIPooling提取每个候选区域对应的特征，并将特征变为统一大小；

​	（2）输入CNN/FC子网络中来增强候选区域的特征；

​	（3）对候选区域进行多分类和回归得到最终检测结果。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 14.28.02.png)

**RCNN系列总结**

附上RCNN系列的对比图，可以从图中看出Faster R-CNN采用RPN进行候选区域的生成，并采用Fast R-CNN延续下来的SoftmaxLoss来分类图像、采用SmoothL1Loss来回归锚框，实现了端到端的训练，大大节省了训练成本，相比于初代的R-CNN显著提高了检测速度。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 14.30.48.png)

**后续改进**

Faster R-CNN已经是RCNN系列的集大成者了，后续也有众多基于Faster R-CNN的改进，其中最经典的改进之一就是FPN了，FPN的提出使得多阶段目标检测网络基本上定型。FPN对Faster R-CNN的改进主要为以下两点：其一是使用分治法（divide-and-conquer），将单个检测层变为多个检测层，每个检测层关联不同尺度的锚框，将目标检测任务分成若干个不同尺度的检测子任务；其二是使用多尺度特征融合，提高了特征的丰富程度。下面来详细讲一下这两点改进是如何做的。

**将单个检测层更改为多个检测层**

在Faster R-CNN中，铺设锚框是在下采样倍数为16的特征层上进行的，即下采样倍数为16的特征层上的每个特征都会关联一系列不同尺度的锚框。采用单个检测层的特征进行锚框关联的问题在于铺设锚框时不够合理，具体表现为小尺度的锚框会铺设的比较稀疏，而大尺度的锚框会铺设的比较稠密，导致小尺度的目标检测难度较大，而大尺度的目标可能会出现较多的误检情况。一个很理所当然的想法就是能不能不同尺度的锚框都像下图中尺度锚框一样铺设间隔刚刚好呢？

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.11.03.png)

在VGG/ResNet等BackBone中，中间的特征层是有很多尺度的，可以取出不同尺度大小的特征层进行锚框关联，如下图所示，更低层的特征层由于感受野较小，可以关联小尺度的锚框，特征层的层数越高，感受野越大，关联锚框的尺度也就可以相应的增大。这样操作可以让大、中、小不同尺度的锚框都具有较合理的铺设间隔。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.16.20.png)

由于不同尺度的特征层都进行了锚框关联，因此需要在这些特征层上分别加入锚框分类、回归子网络，和单个检测层后面接分类、回归子网络类似，对分类采用SoftMax Loss、对回归采用Smooth L1 Loss，采用BP算法即可训练整个网络，使得网络具有不同尺度锚框的分类与回归能力。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.17.25.png)

**多尺度特征融合**

物体检测任务可以等价为物体分类+物体定位任务。对于物体分类任务而言，在网络层数较深时，由于特征层的分辨率较小，对应原图的感受野比较大，特征层具有较为丰富的语义信息，这些特征利于分类任务；对于物体回归任务而言，在网络层数较浅时，由于特征层的分辨率较大，对应原图的感受野较小，图像的细节信息比较丰富，有利于回归任务。

之前提到的VGG\ResNet等Backbone中各特征层的特征无法同时满足这两点，即语义信息和细节信息都丰富。FPN的解决方案是将特征层从深层特征到浅层特征进行融合，让每层的特征都具有丰富的语义和细节信息。

FPN的大致流程如下图所示：

- 下图中左列蓝色块是Faster R-CNN（采用ResNet作为BackBone）的网络结构，每级最后一个Residual Block的输出记为{C1, C2, C3, C4, C5}。FPN用C2-C5参与特征融合（C1的语义还是过于低了)，{C2, C3, C4, C5}分别对应于输入图片的下采样倍数为{4, 8, 16, 32}。
- C5层先经过1×1卷积得到M5层，以改变特征图的通道数(文章中设置d=256，与Faster R-CNN中RPN层的维数相同便于分类与回归)。M5层通过双线性插值或最近邻插值进行上采样（目的是与C4层变为相同大小）再加上C4经过1×1卷积后的特征图（特征图中每一个相同位置元素直接相加），得到M4层。类似过程分别对C3和C2进行，得到M3层和M2层。最终红色的M1-M5层特征图再经过3 x 3卷积进行特征增强，得到最终的P2、P3、P4、P5层特征。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.22.41.png)

下图展示了从图像金字塔到特征金字塔的转变。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.28.25.png)

(a)图像金字塔，对图像进行不同尺度的缩放，对缩放后的图像分别采用CNN进行物体检测，由于各CNN间无法共享参数，需要训练多个CNN网络，且造成了大量的重复计算；

(b)单一检测层，这个和Faster R-CNN中的原理类似，采用最后一层的特征层作为检测层，缺点就是锚框的铺设间隔不够合理；

(c)多个检测层，这个就是上面刚刚讲过的分治法，将多个不同深度的特征层作为检测层，每个检测层关联不同尺度的锚框，可以使得锚框铺设间隔比较合理，但是无法让特征层既包含丰富的语义信息又包含丰富的细节信息；

(d)特征金字塔，对特征层从深层到浅层进行融合，让每层的特征都具有丰富的语义和细节信息。由于每个特征层相对于原图片的感受野具有不同的尺度，因此让每个特征层只关联单一尺度的锚框。具体的，将{P2, P3, P4, P5, P6}这五个特征层分别关联{32×32, 64×64、128×128, 256×256, 512×512}这五种尺度的锚框，每种尺度的锚框具有{1:1, 1:2, 2:1}三种长宽比。（刚刚没有提到P6，P6由P5经过下采样得到，用来处理512大小的候选框。）

**多说几点**

FPN在进行特征融合后，在融合后的特征层上进行了不同尺度物体的分治检测，增加了很少的额外计算，获得了比较明显的性能提升，后续高性能的物体检测算法基本上都会采用FPN。

后续对FPN的特征融合策略也有很多改进的工作，比如PANet、NAS-FPN和EfficientNet中提出的BiFPN也都是比较有意思的且符合直观的。另外还有很有意思的一点，在2021年的一篇CVPR文章《*[YOLOF：You Only Look One-level Feature](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2103.09460)*》中，对FPN进行了深入的分析并得出，FPN中最重要的部分在于对不同尺度的检测目标分而治之的处理思路缓解了优化难的问题，而分治操作在SSD、YOLO v3中就已经出现了，只不过由于当时的BackBone性能不佳，没有过多引起大家的重视，反而是FPN将其发扬光大了。

**性能对比**

R-CNN系列在PASCAL VOC的数据集上的性能对比如下图所示，随着R-CNN系列的发展，检测准确率逐渐提高，运行速度逐渐变快。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.37.11.png)

在Faster R-CNN的主干网络上添加FPN后，在MS-COCO数据集上的性能提升如下图所示，虽然FPN可以显著提升原有网络的检测准确率，但随之带来的是运行速度的下降。

![](./2023-11-07-some-review-about-vision--2D-Object-Detection-Anchor-based-Two-Stage-RCNN-Series/截屏2023-11-08 15.38.42.png)
