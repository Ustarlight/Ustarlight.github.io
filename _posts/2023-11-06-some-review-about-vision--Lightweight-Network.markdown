---
layout: post
title:  "some review about vision - from autonomous cars perspective - lightweight network"
date:   2023-11-06 14:42:45 +0800
categories: share
---

**轻量化网络（一）：神经网络中的显存/显卡带宽/参数量/计算单元/FLOP/FLOPs/MAC/MACs/QPS**

评价一个神经网络，除了看模型的性能（准确率/精度/...）以外，还要考虑在训练/推理时模型占用的内存大小和计算量，毕竟一个效果再好的模型，如果需要很大的内存才能跑起来，或者在推理时需要跑10s才能出结果，对于工业界来说，尤其是自动驾驶感知这种对于推理时间和占用内存大小有硬性要求的场景中是几乎不可接受的。

因此出现了很多直观的模型大小/效率评价指标，本文争取把一些常见的指标一次性讲清楚。

**指标解析**

**显存**：GPU中的显存类似于内存，用于存放模型，数据。显存越大，所能运行的网络也就越大。

**计算单元**：GPU中的计算单元类似于CPU中的核，用来进行数值计算。计算单元的性能越好，进行网络训练/推理的速度就越快。衡量GPU计算单元性能的指标是**FLOPS**，衡量模型计算量的指标是**FLOPs**（注意区分s大小写）。

**显卡带宽**：“显卡带宽”准确离来说是“显存带宽”，是指计算单元与显存之间的数据传输速率，以“字节/秒”或“GB/s”为单位。显存带宽越大，计算单元与显存之间交换等量数据的速率就越快。

**参数量**：指模型中所有需要学习的参数的总数，在论文中有时用#parameters表示，用来衡量一个模型的复杂度，也反应了模型需要占用显存的大小。

**FLOPS**：字母全大写，floating point operations per second，指**每秒浮点运算次数**，可以理解为计算速度，是一个衡量硬件性能的指标。

**FLOPs**：s小写，floating point operations，指**浮点运算数**，可以理解为计算量，用来衡量算法/模型的复杂度。常见的还有MFLOPS (megaFLOPs) = 10^6(百万) FLOPs，GFLOPS (gigaFLOPs)=10^9(十亿) FLOPs，TFLOPS (teraFLOPS) = 10^12(万亿) FLOPs。

**MAC**：memory access cost，指显存/内存访问量，网络各层的特征图以及卷积核的权重都是需要存储在内存中的，在训练/推理过程中，网络需要从内存中读取输入向量或特征图，然后计算点积（涉及从内存中读取参数的权重），最后将计算出的结果写回内存，这涉及大量的内存访问，对模型速度产生影响。

**MACs**：s小写，multiply- accumulate operations的缩写（s表示负数），有的时候也用MAdd表示，指乘加(a+b*c)运算数，1MACs包含一个乘法操作与一个加法操作，通常MACs是FLOPs的2倍。MACs较FLOPs相比不那么常见。

**QPS**：queries per second的缩写，指每秒处理的请求数，该指标常见于高并发系统，用在神经网络中，可以理解为把单块GPU占满（显存/GPU利用率），这张GPU每秒钟能处理多少张图片。在工业界场景中往往由于计算资源受限，考虑的不仅是FLOPs，也不是单纯的inference time，而是把一块GPU打满情况下的QPS。

**如何计算？**

**常规卷积层 conv**

卷积层的输入和输出不是矢量，而是shape为H x W x C的三维特征图，其中H是特征图的高度，W是特征图的宽度，C是每个位置的通道数。假设输入特征图的大小是H_in x W_in x C_in，卷积核的尺寸是K x K，一共有N个卷积核， 输出特征图的大小是H_out x W_out x C_out。

![](./2023-11-06-some-review-about-vision--Lightweight-Network/截屏2023-11-06 16.21.13.png)

**参数量 #parameters**：

单个卷积核的参数量是K x K x C_in，共有C_out个卷积核，所以常规卷积层的参数量是：K x K x C_in x C_out + C_out（每个卷积都有一个偏置项）。但是通常忽略掉偏置项，写成下面的形式。
$$
K * K * C_{in} * C_{out}
$$
**计算量FLOPs**：

输出特征图上每个位置的特征对应于输入特征图上相应位置的一个感受野，输出特征图上的特征是：形状为 K×K×C_in的卷积核与输入特征图相应感受野上的特征依次点乘计算出来的。

单个卷积核一次卷积乘法次数为K×K×C_in ，一次卷积加法次数为K×K×C_in - 1 （相当于10个数相加，共做了9次加法），一个卷积核共进行了H_out×W_out次卷积运算（输出特征图的尺寸H_out × W_out考虑到了stride、padding等因素），共有C_out个卷积核，所以常规卷积层的运算总次数为：(2×K×K×C_in-1)×H_out×W_out×C_out。但是通常只考虑乘法的运算次数，忽略加法，写为下面的形式。
$$
K * K * C_{in} * H_{out} * W_{out} * C_{out}
$$
**内存访问量MAC**：

输入特征图的大小是H_in×W_in×C_in，输出特征图的大小是H_out×W_out×C_out，模型的权重参数量为K×K×C_in×C_out，MAC为上述三项之和，如下式所示。
$$
H_{in} * W_{in} * C_{in}  + H_{out} * W_{out} * C_{out} + K * K * C_{in} * C_{out}
$$
**深度可分离卷积层Depth- wise conv**

假设输入特征图的大小是H_in×W_in×C_in，第一步空间卷积的卷积核尺寸是K×K，一共有C_in个空间卷积，中间特征图的大小是H_out×W_out×C_in，第二步深度卷积的卷积核尺寸是1×1×C_in，一共有C_out个深度卷积，输出特征图的大小是 H_out×W_out×C_out。

![](./2023-11-06-some-review-about-vision--Lightweight-Network/截屏2023-11-06 16.42.10.png)

**参数量 #parameters**

空间卷积的卷积核尺寸是K×K，一共有C_in个空间卷积，参数量为K×K×C_in + C_in；深度卷积的卷积核尺寸是1×1×C_in，一共有C_out个深度卷积，参数量为C_in×C_out + C_out。深度可分离卷积层的总参数量为空间卷积和深度卷积的参数量求和K×K×C_in + C_in + C_in×C_out + C_out，但通常只考虑乘法，忽略加法，如下式所示。
$$
K * K * C_{in} + C_{in} * C_{out}
$$
**计算量 FLOPs**

单个空间卷积核进行一次卷积乘法次数为K×K×1，加法次数为K×K×1 - 1 ，一个空间卷积核共进行了H_out×W_out次卷积运算，共有C_in个卷积核，所以空间卷积的运算量为(2×K×K -1)×H_out×W_out×C_in；单个深度卷积核进行一次卷积乘法次数为1×1×C_in，加法次数为1×1×C_in - 1，一个深度卷积核共进行了H_out×W_out次卷积运算，共有C_out个卷积核，所以深度卷积的运算量为(2×C_in - 1)×H_out×W_out×C_out。

综上所述，深度可分离卷积的运算总次数为：(2×K×K -1)×H_out×W_out×C_in + (2×C_in - 1)×H_out×W_out×C_out，但是通常只考虑乘法的运算次数，忽略加法，写作下式。
$$
K * K * H_{out} * W_{out} * C_{in} + C_{in} * H_{out} * W_{out} * C_{out}
$$
**内存访问量MAC**：

输入特征图的大小是H_in×W_in×C_in，中间特征图的大小是H_out×W_out×C_in，输出特征图的大小是H_out×W_out×C_out，模型的权重参数量为K×K×C_in + C_in×C_out，MAC为上述三项之和，如下式所示。
$$
H_{in} * W_{in} * C_{in}  + H_{out} * W_{out} * C_{in} + H_{out} * W_{out} * C_{out} + K * K * C_{in} + C_{in} * C_{out}
$$
**全连接层**：全连接层的输入与输出都是一维的向量，假设全连接层的输入是C_in，输出是C_out。

![](./2023-11-06-some-review-about-vision--Lightweight-Network/截屏2023-11-06 16.55.09.png)

**参数量 #parameters**

由于有C_in个输入神经元，C_out个输出神经元，全连接层将输入神经元与输出神经元一一连接在一起，参数量为C_in×C_out+C_out（偏置项），忽略偏置项，写作下式。
$$
C_{in} * C_{out}
$$
**计算量 FLOPS**

多个输入神经元对应单个输出神经元进行乘法的次数为C_in，进行加法的次数为C_in - 1，共有C_out个输出神经元，乘加运算总次数为(2×C_in - 1)×C_out，忽略加法写作下式。
$$
C_{in} * C_{out}
$$
**内存访问量MAC**

内存访问量为参数量与输入、输出神经元数量之和，写作下式。
$$
C_{in} + C_{out} + C_{in} * C_{out} + C_{out}
$$
**总结**

我们在衡量一个模型的速度性能的时候，主要考虑的是模型的推理速度，**计算量并不能单独用来评估模型的推理时间，还必须结合硬件特性（算力&带宽），以及访存量来进行综合评估。**并非是计算量越低模型推理越快，**访存量也是作为衡量模型推理速度重要的评价指标**。[田子宸：深度学习模型大小与模型推理速度的探讨](https://zhuanlan.zhihu.com/p/411522457)这篇文章里对影响模型推理速度的因素进行了全面分析，对于如何分析、提升模型的推理速度具有非常大的参考意义，强烈推荐。



**轻量化网络（二）：MobileNet v1/v2/v3**

**MobileNet v1**

自从2017年由谷歌公司提出，MobileNet可谓是轻量级网络中的Inception，经历了一代又一代的更新。成为了学习轻量级网络的必经之路。其实介绍MobileNet v1只有一句话**，MobileNet v1就是把VGG中的标准卷积层换成了深度可分离卷积层。**

深度可分离卷积层做到了**用更少的参数、更少的运算**，达到与标准卷积层相差不大的效果，非常适合部署在移动端等计算资源有限的场景下。

常规卷积层采用卷积核同时捕获空间模式（例如椭圆、线段）和跨通道模式（例如，嘴 + 鼻子 + 眼睛 = 脸），但可分离卷积层假设空间模式和跨通道模式可以单独建模。因此，**深度可分离卷积**就是**将普通卷积拆分成为空间卷积和深度卷积**：**空间卷积**为输入特征图的每个通道分别应用一个单通道卷积核，然后**深度卷积**专门针对跨通道模式建模——它只是一个具有 1×1大小的常规卷积层。

![](./2023-11-06-some-review-about-vision--Lightweight-Network/截屏2023-11-06 17.42.17.png)

标准卷积的操作如下

- 输入一个12×12×3的一个输入特征图，经过5×5×3的卷积核得到一个8×8×1的输出特征图。如果此时我们有256个卷积核，我们将会得到一个8×8×256的输出特征图。

深度可分离卷积的操作如下：

**空间卷积**：将常规的卷积核拆分成为单通道形式，在不改变输入特征图像的通道数的情况下，对每一通道单独进行卷积操作，得到和输入特征图通道数一致的输出特征图。输入12×12×3的特征图，经过5×5×1×3的深度卷积之后，得到了8×8×3的输出特征图。输入特征图和输出特征图的通道数相同，明显可以看出的问题是输出特征图的维度太少，获取不到足够的信息。

**深度卷积**：**就是1×1卷积**。主要作用就是对特征图进行升维和降维，在空间卷积的过程中，我们得到了8×8×3的输出特征图，我们用256个通道数为3的1×1卷积核，对8×8×3的输出特征图进行卷积操作，最终得到8×8×256的输出特征图，与标准卷积操作相同。

![](./2023-11-06-some-review-about-vision--Lightweight-Network/截屏2023-11-06 17.57.05.png)

 定义卷积核的尺寸为K x K，输入特征图的通道数为M，输出特征图的维度数为W x H x N，接下来看看深度可分离卷积与标准卷积相比在参数、计算量上的优势。

标准卷积的参数与计算量

- 卷积核的尺寸是K×K×M，一共有N个，所以标准卷积的参数量是：**K×K×M×N**
- 每个参数都要进行W×H次运算，所以标准卷积的计算量是：**K×K×M×N×W×H**

深度可分离卷积的参数与计算量

- 空间卷积的卷积核尺寸是K×K×1，一共有M个空间卷积，空间卷积的参数量是K×K×M；深度卷积的卷积核尺寸是1×1×M，一共有N个深度卷积，深度卷积的参数量是M×N；所以深度可分离卷积的参数量是：**K×K×M+M×N**
- 每个参数都要进行W×H次运算，所以深度可分离卷积的计算量是：**K×K×M×W×H+M×N×W×H**

通过比较可以得出深度可分离卷积的参数与计算量均下降为标准卷积**的**
$$
\frac{1}{N} + \frac{1}{K^2}
$$
其中N是输出特征图的通道数量，K是卷积核的大小，通常N为64、128、256...不等，而K通常为3（通常使用3×3的卷积核），因此深度可分离卷积的参数与计算量会下降到标准卷积的**九分之一到八分之一**。
