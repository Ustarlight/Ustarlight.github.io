---
layout: post
title:  "2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet"
date:   2023-11-08 16:47:45 +0800
categories: share
---

整个RCNN系列的算法都始于锚框设计，锚框设计的好坏一定程度上决定了检测算法的性能上限。而设计锚框用到了众多的超参数，这带来了调参困难、使用起来不够简便等问题。

锚框设计有三个超参数：

1. 锚框的关联层：选择哪些特征层作为关联锚框的检测层
2. 锚框的大小：每个检测层上关联锚框的尺度大小
3. 锚框的比例：每个检测层上关联锚框的长宽比例

锚框设计好后，训练过程中需要对锚框进行匹配，划分正负样本。锚框匹配有两个超参数：

1. 选取正样本的IoU阈值：大于等于该IoU阈值的锚框是正样本
2. 选取负样本的IoU阈值：小于等于该IoU阈值的锚框是负样本

在待检测的物体**类别**、**场景**、**尺度**、**比例**不同时，可能都需要人为地根据经验对锚框超参数进行设计和调节，经验不足的新手很难达到较好的应用效果。且在应用基于锚框的检测算法时，最耗时的地方就在于锚框超参数的调整。

**CornerNet的提出**

无需锚框的检测算法就是为了解决上述提到的难调参等问题，提出了全新的不依赖锚框的物体检测流程，并可以达到与基于锚框的检测算法相似的性能。消除了锚框相关的超参数，意味着算法的超参数更少，使得在自己面临的检测场景中应用算法更加方便。

在ECCV 2018上提出的CornerNet属于这一类算法的开山鼻祖，通过寻找物体边界框的左上角点和右下角点，实现物体的定位，并对角点进行分类，得到物体的类别。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 16.57.35.png)

**CornerNet详解**

CornerNet的大致检测流程如下：

1. 输入一张图像；
2. 提取特征：采用特殊的CNN结构提取特征；
3. 角点检测：将特征分别送入两个检测网络，一个用来检测物体的左上角点，一个用来检测物体的右下角点；
4. 角点配对：将角点映射到同一个空间中，将距离比较近的角点看作是属于同一个物体的角点；
5. 输出结果：将属于同一个物体的左上角点和右下角点绘制在图像中，得到物体的边界检测框。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 16.56.04.png)

CornerNet的算法框架如下，接下来我们来详细讲一下每个模块都是怎样工作的以及用途是什么。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 16.58.36.png)

### 数据增广

首先对输入图像进行数据增广，与SSD类似，增广的方式有随机颜色抖动、随机裁剪、随机扩充、随机水平翻转，对图像进行PCA白化操作，以及不等比例地缩放至511×511大小。

### 基础网络

采用2级Hourglass Network输出128×128的特征用于后续预测。值得一提的是该网络属于FPN的起源，于ECCV 2016上提出，其作者和CornerNet作者是一个研究团队，

### 角点预测模块

以左上角点的预测模块举例，详细结构如下图所示，其中灰色模块由3×3的Conv+BN+ReLU组成。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.00.13.png)

角点预测模块的输入是Backbone输出的128×128维的特征，首先分别经过两个3×3 Conv-BN-ReLU层，并分别通过一个额外的处理——Corner Pooling后进行求和，求和后的特征送入一个3×3 Conv-BN层，与原始Backbone输出的128×128维特征经过一个1×1 Conv-BN层强化后的特征相加，再通过一个ReLU和3×3 Conv-BN-ReLU层，得到最终强化后的角点特征，之后这个角点特征会分别送入三个不同的模块进行处理，分别为预测角点的类别、预测角点的编码、预测角点的偏移。接下来分别详细介绍各个模块的内容。

### Corner Pooling

之所以引入Corner Pooling这个操作，是由于一个物体边界框的左上角点和右下角点大多数处于背景区域中，并不在物体上，缺少关键的物体特征信息。通过Corner Pooling的操作，增强左上角点和右下角点这两个角点的特征，利于后续任务的进行。Corner Pooling的具体操作如下，将特征图上的所有特征的值替换为以特征为起点，**水平向右遇到的最大值**以及**竖直向下遇到的最大值**之**和**。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.07.11.png)

作者采用了一种高效的方法，即分别从右至左、从下至上处理特征，将沿着当前处理方向上遇到的最大值作为替换值即可快速实现corner pooling。这样每行或者每列只需要进行少量的判断即可，不需要每个点都要判断所沿方向上的所处行、列的最大值，加速了处理效率。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.08.27.png)

**角点类别预测分支**

该分支的作用就是预测角点的类别（比如说80类的COCO数据集，该分支需要预测当前角点属于80类中的哪一类）。在一个常见的目标检测数据集中，GT如下图中红框所示，GT的左上角和右下角对应的位置就是角点的正样本，对应 y=1 ，其他地方都是负样本 y=0 。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.10.43.png)

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.11.38.png)

这样处理后，样本的标签值就不再是非0即1的了，在采用Focal Loss的基础上，根据样本的标签值对损失函数进行加权（下式中除了红框以外的表达式就是Focal Loss）。当样本的标签值是负样本（ y≠1 ）时，如果该样本距离正样本过近，红框中的表达式就会得出一个比较小的值，降低了距离正样本过近的那些负样本的权重，相当于在训练中将这些距离过近的负样本尽量忽略掉；当该样本距离正样本距离足够远的时候，红框中的表达式就会得出较接近1的值，相当于在训练中把这些负样本正常训练就可以了。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.14.05.png)

### 角点编码预测分支

单单找到角点的正确分类和位置还不够，需要知道左上角点和右下角点的配对关系，如果配对错了，检测结果也是错误的。该分支的作用是预测角点在另一个空间中的编码，以用来后续的两两角点配对。

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.23.09.png)

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.24.32.png)

![](./2023-11-08-some-review-about-vision--2D-Object-Detection-Anchor-free-CornerNet/截屏2023-11-08 17.26.22.png)

### Loss函数定义

综上所述，CornerNet的损失函数是由三部分组成的。
$$
L = L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off}
$$

$$
L_{det}
$$

是角点分类的损失函数，用来预测角点所在的类别，其特色在于采用Focal Loss的基础上，降低了距离正样本过近的那些负样本的权重以消除歧义性。
$$
L_{pull} 和 L_{push}
$$
是角点配对时用到的损失函数，其思想在于一个图像中不止包含一个物体，会找到众多物体的左上角点和右下角点，需要对属于同一物体的左上角点和右下角点进行配对，先将所有角点映射到同一空间中，采用
$$
L_{pull}
$$
 使得属于同一个物体的角点之间距离最近，采用
$$
L_{push}
$$
使得不属于同一个物体的角点之间距离足够远。
$$
L_{off}
$$
是角点偏移预测时用到的损失函数，其思想在于原图像的大小经过下采样4倍并向下取整后，导致原图像上16个像素对应了特征图上的1个像素无法找到原图像精确的角点位置。采用
$$
L_{off}
$$
 预测下采样取整时的角点偏移量，在映射回原图时加上这个偏移量，即可得到精确的角点位置。

**总结**

CornerNet是Anchor-Free Object Detection的开创性文章，其思路简洁有效，开辟了物体检测的一个新方向。CornerNet的思想取自于人体姿态估计的自底向上思想，即先通过图片得到对应物体的左上角和右下角两个关键点，再根据关键点的相似度拼接出检测到的不同的物体。

但CornerNet仍有许多不足，仅以左上和右下两个角进行匹配，而忽略了物体的中心特征，容易产生假阳样本。另外做互相遮盖的物体检测中很多物体并没有明显的角的特征，可能会造成CornerNet在检测覆盖物体的时候表现不理想。

后续出现的CenterNet、Deformable系列和RepPoints系列算是对CornerNet的一系列缺点进行了改进，逐渐使得Anchor-Free方法在精度、效率上都大大提高。

