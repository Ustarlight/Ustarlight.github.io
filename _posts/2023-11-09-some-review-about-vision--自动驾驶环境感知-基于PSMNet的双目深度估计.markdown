---
layout: post
title:  "2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计"
date:   2023-11-09 13:52:45 +0800
categories: share
---

**背景介绍**

双目深度估计是双目3D感知的任务之一，主要目标是通过两个相机估计场景的深度信息。得到场景的深度信息后，就可以将图像中的各个点结合深度信息转换为点云（如Pseudo-Lidar）算法，再通过点云目标检测/分割方法进行3维物体检测。

双目深度估计算法有以下4个优势：

1. 不需要先验知识，单目3D算法往往依赖于先验知识和几何约束，如地面是平的，车辆的高度为0等，但是在实际场景中，经常无法满足这些假设；
2. 基于深度学习的算法如PSMNet可以在大规模、高质量、高多样性的数据集上进行训练，提升算法在实际复杂多变场景下的鲁棒性；
3. 通过计算“视差”可以恢复场景的深度信息，解决了单目系统中由于透视变换带来的深度歧义性；
4. 只要目标点出现在两个相机中，都可以得到该目标点的深度信息，不依赖于物体检测的结果，对场景中的任意障碍物（如未标注的路桩，路障等）均有效。

同时双目3D感知算法也有一定的劣势：

- 硬件层面：两个摄像头需要进行位置上的精确配准才能得到精确的深度估计结果，一般在出厂时配准精度是最高的，但在车辆行驶过程中，由于颠簸、震动等会使得摄像头的位置发生偏移，需要设计在线配准算法确保配准参数的精确，这带来了额外的计算量，此外在线配准算法的设计也非常具有挑战性；
- 软件层面：算法需要同时处理两个摄像头的数据，计算复杂度较高，需要配备更大算力的芯片才能保证实时计算。

**基本原理**

双目深度估计背后涉及到的原理可以用如下一张图表示清楚，在图中Z轴表示的是车身纵向的维度，X轴表示的是车身侧向的维度，由于双目系统中的左、右两个相机被安装在同一高度，这里不需要高度方向上的维度。

将左、右两个相机在X轴上的距离记作基线长度 B，两个相机具有相同的焦距 f，为了描述方便将两个相机的成像平面从后方转移至前方，由于相似三角形，对接下来的推导和结论不会有任何影响。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 14.30.26.png)

目标点 o 在三维空间中的横向坐标与深度坐标表示为 (x, z)，由于两个相机在X方向上的位置差异不同，目标点o经过两个相机中成像后，在X方向上的坐标
$$
x_l, x_r
$$
也是不一样的，将这个差异记为视差
$$
d = x_l - x_r
$$
实际双目系统中的视差如下图所示，对于同一个电线杆，在相机中 X 方向上的坐标并不相同。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.19.59.png)

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.20.50.png)

由此可见双目深度估计的关键就是估计视差，即对于左图中的每个像素点，都需要在右图中找到与其匹配的点。在自动驾驶系统中，需要估计的场景的深度范围是有限的，如最远90m至最近1.7m。有了深度的范围后，可以将其转换为视差的范围，如10个像素至1个像素 ，由此得到了10种不同的视差级别，意味着每个像素点都有10种视差的选择，对于每一个可能的视差（范围有限），计算匹配误差。得到三维的误差数据称为Cost Volume（图像是2维的，视差级别额外增加了1维，因此误差数据是三维的）。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.33.26.png)

举例说明，位于左侧输入图像上的一个像素点，在右侧输入图像中有3种可能的视差选择，对应了右侧输入图像中的3个像素点，对于每一种视差，计算匹配误差，得到Cost Volume。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.34.23.png)

在计算匹配误差时，最简单的方式就是计算像素值之间的差异作为匹配误差，更好的方式是考虑像素点邻域的局部区域，比如对邻域内所有像素值的差进行求和作为匹配误差，更进一步可以通过神经网络自动学习像素点邻域内的有效特征，通过这些自动提取的视觉特征计算匹配误差。

通过Cost Volume可以得到每个像素处的视差（对应最小匹配误差的视差级别），进而得到每个像素点处的深度值。

由此可见，双目深度估计的关键是计算匹配误差，而计算匹配误差的关键在于特征提取，本文的主角PSMNet也就出场了。

[Pyramid Stereo Matching Network](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.08669)简称PSMNet，是2018年提出的一个非常经典的基于神经网络的双目深度估计方法。PSMNet的整体网络结构如下图所示，PSMNet流程如下：

1. 输入图像：来自左、右两个相机的图像；
2. 特征提取：采用参数共享的卷积网络进行特征提取，包含用于提取多分辨率特征的下采样和金字塔结构 ，也包括在扩大感受野的同时保持特征图分辨率不变的空洞卷积；
3. Cost Volume构建：采用左、右两个图像经过卷积网络后得到的特征图构建Cost Volume；
4. 特征融合：采用3D卷积提取左、右特征图及不同视差级别之间的信息，得到特征融合后的Cost Volume；
5. 视差计算：将特征融合后的Cost Volume上采样到原始分辨率，找到匹配误差最小的视差值。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.37.52.png)

### CNN模块

输入图像先经过CNN模块实现初步的特征提取，CNN模块的架构示意图如下图所示，通过一系列的下采样和空洞卷积，保证网络可以得到较大感受野的特征。

- CNN模块的前3层为标准的kernel_size=3×3, channel=32卷积，conv0_1的步长为2将原始分辨率下采样到1/2；
- conv1_x和conv2_x采用了Resnet中经典的网络结构residual block，分别重复了3次和16次，conv2_x中的通道数为64，conv2_1的步长为2将分辨率下采样到原始分辨率的1/4；
- conv3_x和conv4_x也采用了residual block，都重复了3次，channel数为128，需要注意的是residual block中的常规cnn并替换为了空洞卷积（膨胀率分别为2和4），用于在提升感受野的同时保持特征图分辨率不变。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.41.35.png)

### SPP模块

SPP模块的全称是Specail Pyramid Pooling，用于提取像素点周围不同尺度的邻域信息。SPP的输入是经过CNN模块处理后的1/4 H×1/4 W×128的特征图，SPP中包含4个分支，对应着大小为64×64、32×32、16×16、8×8的average pooling，pooling的尺寸越大，对应着在原图中的感受野就越大，意味着获取到的特征更偏向于全局特征，同理pooling的尺寸越小，意味着获取到的特征更偏向于局部特征。

通过尺寸不同的pooling可以获取位于不同全局层次的邻域特征，这些特征会依次经过kernel_size=3×3, channel=32的卷积层，通过上采样（双线性插值）恢复到SPP的输入特征分辨率1/4 H×1/4 W，并与CNN模块在conv2_16和conv4_3的输出在channel上进行拼接，得到1/4 H×1/4 W×320(64+128+32*4)的特征图，作为SPP的输出。SPP的输出特征图既包含了不同扩大倍数感受野的特征，也包含了具有不同尺度邻域信息的特征。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.46.43.png)

SPP输出特征图的大小为1/4 H×1/4 W×320，为了减小特征图的通道数，依次采用了kernel_size=3×3, channel=128和kernel_size=1×1, channel=32的卷积将SPP的输出特征图大小变为1/4 H×1/4 W×32。

### 构建Cost Volume

经过CNN模块和SPP模块，左、右两个相机的输入图像（大小为H×W×3）被转换为两个特征图（大小为1/4 H×1/4 W×32），在这两个特征图上有着丰富的全局与局部的信息。该步骤就是通过这两个特征图构建Cost Volume，具体构建过程如下图所示。

在该图中橙色的部分表示来自左侧的特征图，蓝色的部分表示来自右侧的特征图，假设一共有D个视差等级，视差等级1对应着1个像素、视差等级2对应着2个像素...，以左侧特征图为基准，将右侧特征图向左平移相应像素，共移动D次，得到D个左、右特征图的组合，将移动后的特征图在Channel上进行拼接，将右侧特征图多余的部分裁剪掉，空缺的部分补0。这样对于每一个等级的视差图，大小为1/4 H×1/4 W×64(2×32)，由于当前特征图的下采样倍数是1/4，视差等级也会进行相应比例的下采样，即一共有1/4D个大小为1/4 H×1/4 W×64的Cost Volume，可以将Cost Volume看作是一个4D Tensor，维度是1/4 H×1/4 W×1/4 D×64。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.48.14.png)

### 特征融合

在上一步骤中，得到了大小为1/4 H×1/4 W×1/4 D×64的4维Tensor，在这个4维Tensor中，既包含了左、右图像之间的信息，也包含了不同视差等级之间的信息，接下来采用三维卷积对这个4维Cost volume中的信息进行进一步融合和提取，得到三维的Cost Volume。PSMNet中提出了2种三维卷积的搭建版本。

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.49.01.png)

第一种是基础版本：

- 采用residual block的结构堆叠12个3×3×3的三维卷积；
- 在最后一层三维卷积的后面接了一个上采样层，将特征图的分辨率上采样到H×W×D×C，并采用kernel_size=3×3×3, channel=1的三维卷积将特征图的channel数变为1，由此相当于得到了一个3维Tensor（大小为H×W×D）

第二种是Stacked hourglass版本，网络在进行实际性能测试时也是采用的这个版本：

- 重复采用3次自上而下/自下而上的沙漏网络，在每个沙漏网络中特征图的分辨率都先下降至1/16 H×1/16 W×1/16 D再上采样回1/4 H×1/4 W×1/4 D，其中还包含了一些沙漏网络特征图之间的连接，在此就不再过多赘述了；
- 3个沙漏网络的最后一层都接了上采样层将特征图的分辨率上采样回H×W×D，同样分别采用kernel_size=3×3×3, channel=1的三维卷积将特征图的channel数变为1，由此相当于得到了一个3个3维Tensor（大小为H×W×D）。在网络训练过程中这3个3维Tensor都会参与Loss的计算，在网络推理阶段只采用最后一层沙漏网络输出的Tensor。

### 视差计算

在上一步特征融合中，得到的大小为H×W×D的3维Tensor其实就是具有D个视差等级的Cost Volume，在传统视差计算中，只需要对于每个像素点，在视差等级这个维度上将具有最小匹配误差的视差作为该像素点的视差即可，相当于是对所有像素点在视差等级维度上进行Argmin操作。但是Argmin这个操作是不可微的，无法采用BP训练，而且这个操作得到的视差都是整数，无法得到亚像素的视差估计。

因此PSMNet采用了[End-to-End Learning of Geometry and Context for Deep Stereo Regression](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.04309.pdf)提出的Soft ArgMin来替代传统的ArgMin：

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.53.09.png)

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.55.01.png)

**Loss**

![](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/截屏2023-11-09 15.59.08.png)

## 总结

PSMNet是一种端到端的双目深度估计方法，该网络的亮点在于在Backbone处添加了空洞卷积以增大感受野并采用pyramid pooling提取不同尺度的邻域信息生成初步的Cost Volume，然后采用stacked hourglass对Cost Volume中包含的左、右图像之间的信息和不同视差等级之间的信息进行进一步融合，PSMNet的消融实验也证实了这些方法对于性能提升确实是有效的。

![img](./2023-11-09-some-review-about-vision--自动驾驶环境感知-基于PSMNet的双目深度估计/v2-40b8de8fb5bad4857ce793afd415719b_1440w.webp)

PSMNet的效果图如下，在视差图中颜色越接近蓝色表示视差越大，深度越小；颜色越接近红色表示视差越小，深度越大。

![img](../figures/v2-48ec7792fd7093f82874ceb58b334b8d_1440w.webp)

原图

![img](../figures/v2-5a63d61af6495fd60482b39a425a140c_1440w.webp)

视差图
